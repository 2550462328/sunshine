**1. 关于数据扩容?**

从putVal源代码中我们可以知道，当插入一个元素的时候size就加1，若size大于threshold的时候，就会进行扩容。

假设我们的capacity大小为32，loadFator为0.75,则threshold为24 = 32 * 0.75，此时，插入了25个元素，并且插入的这25个元素都在同一个桶中，桶中的数据结构为红黑树，则还有31个桶是空的，也会进行扩容处理，其实，此时，还有31个桶是空的，好像似乎不需要进行扩容处理，但是是需要扩容处理的，因为此时我们的capacity大小可能不适当。

我们前面知道，扩容处理会遍历所有的元素，时间复杂度很高；前面我们还知道，经过一次扩容处理后，元素会更加均匀的分布在各个桶中，会提升访问效率，所以说尽量避免进行扩容处理，也就意味着，遍历元素所带来的坏处大于元素在桶中均匀分布所带来的好处。　



**2. 在resize()扩容时为什么通过e.hash & oldCap是否为0来进行rehash？**

举例说明一下，下面n为table的长度，图（a）表示扩容前的key1和key2两种key确定索引位置的示例，图（b）表示扩容后key1和key2两种key确定索引位置的示例，其中hash是key对应的哈希与高位运算结果（没有和n-1做&运算前）。

![img](http://pcc.huitogo.club/e6436173d50d92b4a15332653ef37474)



元素在重新计算hash之后，因为n变为2倍，那么n-1的mask范围在高位多1位(红色)，因此新的index就会发生这样的变化：

![img](http://pcc.huitogo.club/f4706b202ce78c2f4e91e46c191f375c)

因此，我们在扩充HashMap的时候，不需要像JDK1.7的实现那样重新计算hash，只需要看看原来的hash值新增的那个bit是1还是0就好了，是0的话索引没变，是1的话索引变成“原索引+oldCap”，也就是通过e.hash & oldCap来判断新增的bit是0还是1。

**由于新增的1bit是0还是1可以认为是随机的，因此resize的过程，均匀的把之前的冲突的节点分散到新的bucket了**。

JDK1.7中rehash的时候，旧链表迁移新链表的时候，如果在新表的数组索引位置相同，则链表元素会倒置，但是从上面过程分析后**JDK1.8中hashMap进行rehash时不会倒置。**